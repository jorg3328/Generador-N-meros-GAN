{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9pEPWVkWVU3",
        "outputId": "0b76039a-c706-4757-85ea-9dbc060c614b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "¿Usando GPU?: True\n"
          ]
        }
      ],
      "source": [
        "# BLOQUE 1: IMPORTACIONES Y CONFIGURACIÓN\n",
        "\n",
        "import argparse # 'argparse' nos ayuda a manejar opciones, aunque aquí usaremos variables directas.\n",
        "import os # 'os' nos permite interactuar con el sistema operativo (crear carpetas, leer archivos).\n",
        "import numpy as np # 'numpy' es la calculadora científica fundamental de Python (maneja matrices de números).\n",
        "import math # 'math' nos da funciones matemáticas básicas.\n",
        "import torchvision.transforms as transforms # 'torchvision.transforms' contiene herramientas para editar imágenes (rotar, cambiar tamaño, normalizar).\n",
        "from torchvision.utils import save_image# 'save_image' es una función específica para guardar nuestros resultados como .png o .jpg.\n",
        "from torch.utils.data import DataLoader # 'DataLoader' es el encargado de cargar los datos en lotes (paquetes) para no llenar la memoria RAM.\n",
        "from torchvision import datasets # 'datasets' tiene bases de datos famosas listas para usar (como MNIST).\n",
        "from torch.autograd import Variable # 'Variable' es un envoltorio antiguo de PyTorch, pero útil en versiones legacy para decir \"esto se va a entrenar\".\n",
        "import torch.nn as nn # 'torch.nn' contiene los bloques de construcción de redes neuronales (capas densas, convoluciones).\n",
        "import torch.nn.functional as F # 'torch.nn.functional' contiene funciones sin estado (como activaciones o pérdidas).\n",
        "import torch # 'torch' es la librería principal (el cerebro de todo).\n",
        "\n",
        "\n",
        "# Creamos una carpeta llamada \"images\" para guardar el progreso del generador.\n",
        "# exist_ok=True evita que el código falle si la carpeta ya existe.\n",
        "os.makedirs(\"images\", exist_ok=True)\n",
        "\n",
        "# --- HIPERPARÁMETROS (Las perillas que ajustamos) ---\n",
        "n_epochs = 50        # Número de veces que el modelo verá todos los datos (Ciclos de entrenamiento).\n",
        "batch_size = 64      # Cuántas imágenes procesa la IA al mismo tiempo (Lotes).\n",
        "lr = 0.0002          # Learning Rate: Qué tan rápido aprende (muy alto = inestable, muy bajo = lento).\n",
        "b1 = 0.5             # Un parámetro interno del optimizador Adam (maneja el impulso).\n",
        "b2 = 0.999           # Otro parámetro interno de Adam.\n",
        "n_cpu = 8            # Cuántos núcleos del procesador usar para cargar datos.\n",
        "latent_dim = 100     # Tamaño del \"ruido\" inicial. Es la semilla de la imaginación de la IA.\n",
        "img_size = 28        # Tamaño de la imagen (MNIST son números de 28x28 píxeles).\n",
        "channels = 1         # Canales de color: 1 = Blanco y Negro, 3 = RGB.\n",
        "sample_interval = 400 # Cada cuántos pasos guardamos una imagen de prueba.\n",
        "\n",
        "# Definimos el tamaño exacto de la imagen multiplicando sus dimensiones.\n",
        "# 1 * 28 * 28 = 784 píxeles totales por imagen.\n",
        "img_shape = (channels, img_size, img_size)\n",
        "\n",
        "# Verificamos si Google Colab nos prestó una GPU (Tarjeta Gráfica).\n",
        "# Si 'cuda' es True, el entrenamiento será rapidísimo. Si es False, será lento.\n",
        "cuda = True if torch.cuda.is_available() else False\n",
        "print(f\"¿Usando GPU?: {cuda}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# BLOQUE 2: DEFINICIÓN DEL GENERADOR\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        # Inicializamos la clase padre (necesario en Python para heredar funciones).\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        # Definimos las capas de la red como una lista de pasos secuenciales.\n",
        "        def block(in_feat, out_feat, normalize=True):\n",
        "            layers = [nn.Linear(in_feat, out_feat)] # Capa lineal: conecta todas las entradas con las salidas.\n",
        "            if normalize:\n",
        "                # BatchNormalization: Reajusta los valores matemáticos para que el aprendizaje no se descontrole.\n",
        "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
        "            # LeakyReLU: Una función de activación que deja pasar señales negativas pequeñas (evita neuronas muertas).\n",
        "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "            return layers\n",
        "\n",
        "        # Construimos el modelo apilando bloques.\n",
        "        # La red va creciendo: Entra ruido (100) -> 128 -> 256 -> 512 -> 1024 neuronas.\n",
        "        self.model = nn.Sequential(\n",
        "            *block(latent_dim, 128, normalize=False),\n",
        "            *block(128, 256),\n",
        "            *block(256, 512),\n",
        "            *block(512, 1024),\n",
        "            # Capa final: Convierte las 1024 neuronas en los 784 píxeles de la imagen.\n",
        "            nn.Linear(1024, int(np.prod(img_shape))),\n",
        "            # Tanh: Aplasta los valores para que estén entre -1 y 1 (formato estándar de imágenes en IA).\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    # La función 'forward' define cómo pasan los datos por la red.\n",
        "    def forward(self, z):\n",
        "        # Pasamos el ruido 'z' por el modelo definido arriba.\n",
        "        img = self.model(z)\n",
        "        # Reorganizamos la lista de números (784) en forma de imagen (1 canal, 28 alto, 28 ancho).\n",
        "        img = img.view(img.size(0), *img_shape)\n",
        "        return img"
      ],
      "metadata": {
        "id": "GL2u8aYoXQAy"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BLOQUE 3: DEFINICIÓN DEL DISCRIMINADOR\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            # Capa 1: Recibe la imagen aplanada (784 píxeles) y la comprime a 512 neuronas.\n",
        "            nn.Linear(int(np.prod(img_shape)), 512),\n",
        "            # LeakyReLU: Activación para aprender patrones complejos.\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            # Capa 2: De 512 a 256 neuronas.\n",
        "            nn.Linear(512, 256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            # Capa Final: De 256 a 1 sola neurona.\n",
        "            # ¿Por qué 1? Porque la respuesta es binaria: 0 (Falso) o 1 (Real).\n",
        "            nn.Linear(256, 1),\n",
        "            # Sigmoid: Convierte el número final en una probabilidad entre 0% y 100%.\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        # Aplanamos la imagen: de (1, 28, 28) a una fila larga de (784).\n",
        "        img_flat = img.view(img.size(0), -1)\n",
        "        # Pasamos la imagen plana por el modelo para obtener el veredicto (validez).\n",
        "        validity = self.model(img_flat)\n",
        "        return validity"
      ],
      "metadata": {
        "id": "7OdZ46PzXiYR"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BLOQUE 4: INICIALIZACIÓN\n",
        "\n",
        "# Calculamos la \"Pérdida\" (Loss). Usamos BCELoss (Binary Cross Entropy).\n",
        "# Es la fórmula matemática que mide qué tan mal se equivocó el modelo (Policía vs Ladrón).\n",
        "adversarial_loss = torch.nn.BCELoss()\n",
        "\n",
        "# Instanciamos (creamos) los dos modelos.\n",
        "generator = Generator()\n",
        "discriminator = Discriminator()\n",
        "\n",
        "# Si tenemos GPU (cuda), movemos los modelos y la función de pérdida a la tarjeta gráfica.\n",
        "if cuda:\n",
        "    generator.cuda()\n",
        "    discriminator.cuda()\n",
        "    adversarial_loss.cuda()\n",
        "\n",
        "# --- CARGA DE DATOS ---\n",
        "# Configuramos el DataLoader.\n",
        "dataloader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST(\n",
        "        \"../../data/mnist\", # Carpeta donde se guardarán los datos.\n",
        "        train=True,         # Queremos los datos de entrenamiento.\n",
        "        download=True,      # Descargarlos si no existen.\n",
        "        transform=transforms.Compose(\n",
        "            # Redimensionamos a 28x28 (por seguridad).\n",
        "            [transforms.Resize(img_size),\n",
        "             # Convertimos la imagen a Tensor (formato numérico de PyTorch).\n",
        "             transforms.ToTensor(),\n",
        "             # Normalizamos: convertimos los píxeles de [0,1] a [-1,1] para que coincida con Tanh.\n",
        "             transforms.Normalize([0.5], [0.5])]\n",
        "        ),\n",
        "    ),\n",
        "    batch_size=batch_size, # Tamaño del paquete.\n",
        "    shuffle=True,          # Barajamos los datos para que la IA no memorice el orden.\n",
        ")\n",
        "\n",
        "# Optimizadores: Son los algoritmos que actualizan los \"pesos\" (conocimiento) de la red.\n",
        "# Usamos Adam, que es muy popular y eficiente.\n",
        "optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(b1, b2))\n",
        "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(b1, b2))\n",
        "\n",
        "# Definimos el tipo de dato Tensor según si usamos GPU o CPU.\n",
        "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OpXUG4gKXmbh",
        "outputId": "fa03a4c7-cb6f-4e8d-cd55-b03008aa2d3f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 18.7MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 502kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 4.62MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 9.56MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# BLOQUE 5: ENTRENAMIENTO\n",
        "\n",
        "print(\"Iniciando entrenamiento... (Esto puede tardar unos minutos)\")\n",
        "\n",
        "# Ciclo principal: Repetimos por el número de 'epochs'.\n",
        "for epoch in range(n_epochs):\n",
        "    # Ciclo interno: Repetimos por cada lote de imágenes en el dataset.\n",
        "    for i, (imgs, _) in enumerate(dataloader):\n",
        "\n",
        "        # --- PREPARAR ETIQUETAS ---\n",
        "        # Creamos etiquetas de \"Verdad\" (1.0) para decir que una imagen es Real.\n",
        "        valid = Variable(Tensor(imgs.size(0), 1).fill_(1.0), requires_grad=False)\n",
        "        # Creamos etiquetas de \"Mentira\" (0.0) para decir que una imagen es Falsa.\n",
        "        fake = Variable(Tensor(imgs.size(0), 1).fill_(0.0), requires_grad=False)\n",
        "\n",
        "        # Convertimos las imágenes reales al formato adecuado (Tensor).\n",
        "        real_imgs = Variable(imgs.type(Tensor))\n",
        "\n",
        "        # -----------------\n",
        "        #  ENTRENAR GENERADOR (El Falsificador)\n",
        "        # -----------------\n",
        "        optimizer_G.zero_grad() # Limpiamos los gradientes (errores) anteriores.\n",
        "\n",
        "        # Generamos ruido aleatorio (la \"semilla\" creativa).\n",
        "        z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], latent_dim))))\n",
        "\n",
        "        # El Generador crea imágenes falsas a partir del ruido.\n",
        "        gen_imgs = generator(z)\n",
        "\n",
        "        # Calculamos qué tan bien engañó al discriminador.\n",
        "        # Queremos que el discriminador diga que estas imágenes son Válidas (1.0).\n",
        "        g_loss = adversarial_loss(discriminator(gen_imgs), valid)\n",
        "\n",
        "        # Backpropagation: Calculamos cómo ajustar los pesos para mejorar el engaño.\n",
        "        g_loss.backward()\n",
        "        # Actualizamos el cerebro del Generador.\n",
        "        optimizer_G.step()\n",
        "\n",
        "        # ---------------------\n",
        "        #  ENTRENAR DISCRIMINADOR (El Policía)\n",
        "        # ---------------------\n",
        "        optimizer_D.zero_grad() # Limpiamos gradientes.\n",
        "\n",
        "        # Medimos qué tan bien clasifica las imágenes REALES (debe decir 1.0).\n",
        "        real_loss = adversarial_loss(discriminator(real_imgs), valid)\n",
        "\n",
        "        # Medimos qué tan bien clasifica las imágenes FALSAS (debe decir 0.0).\n",
        "        # Usamos .detach() para no afectar al generador en este paso.\n",
        "        fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake)\n",
        "\n",
        "        # Promediamos ambas pérdidas (detectar real y detectar falso).\n",
        "        d_loss = (real_loss + fake_loss) / 2\n",
        "\n",
        "        # Backpropagation del policía.\n",
        "        d_loss.backward()\n",
        "        # Actualizamos el cerebro del Discriminador.\n",
        "        optimizer_D.step()\n",
        "\n",
        "        # --- REPORTE DE PROGRESO ---\n",
        "        # Imprimimos el estado cada cierto tiempo para saber que no se congeló.\n",
        "        batches_done = epoch * len(dataloader) + i\n",
        "        if batches_done % sample_interval == 0:\n",
        "            print(f\"[Epoch {epoch}/{n_epochs}] [Batch {i}/{len(dataloader)}] [D loss: {d_loss.item():.4f}] [G loss: {g_loss.item():.4f}]\")\n",
        "            # Guardamos una muestra de las imágenes generadas para ver cómo mejora.\n",
        "            save_image(gen_imgs.data[:25], f\"images/{batches_done}.png\", nrow=5, normalize=True)\n",
        "\n",
        "# --- GUARDADO FINAL ---\n",
        "# Al terminar todas las épocas, guardamos el \"cerebro\" entrenado en un archivo.\n",
        "# Este archivo .pth es el que usaremos en la App de Gradio.\n",
        "torch.save(generator.state_dict(), 'generador_mnist.pth')\n",
        "print(\"¡Entrenamiento completado! Modelo guardado como 'generador_mnist.pth'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yd1E4gGqXsNH",
        "outputId": "3a46e095-78f3-4084-c89c-d709b763f992"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iniciando entrenamiento... (Esto puede tardar unos minutos)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3665318708.py:12: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /pytorch/torch/csrc/tensor/python_tensor.cpp:78.)\n",
            "  valid = Variable(Tensor(imgs.size(0), 1).fill_(1.0), requires_grad=False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 0/50] [Batch 0/938] [D loss: 0.7065] [G loss: 0.6851]\n",
            "[Epoch 0/50] [Batch 400/938] [D loss: 0.4828] [G loss: 0.9234]\n",
            "[Epoch 0/50] [Batch 800/938] [D loss: 0.4045] [G loss: 1.0325]\n",
            "[Epoch 1/50] [Batch 262/938] [D loss: 0.4830] [G loss: 1.0975]\n",
            "[Epoch 1/50] [Batch 662/938] [D loss: 0.5366] [G loss: 1.9022]\n",
            "[Epoch 2/50] [Batch 124/938] [D loss: 0.4550] [G loss: 1.0367]\n",
            "[Epoch 2/50] [Batch 524/938] [D loss: 0.2896] [G loss: 2.3139]\n",
            "[Epoch 2/50] [Batch 924/938] [D loss: 0.3697] [G loss: 1.9196]\n",
            "[Epoch 3/50] [Batch 386/938] [D loss: 0.2918] [G loss: 2.5375]\n",
            "[Epoch 3/50] [Batch 786/938] [D loss: 0.5577] [G loss: 3.2851]\n",
            "[Epoch 4/50] [Batch 248/938] [D loss: 0.1696] [G loss: 2.0513]\n",
            "[Epoch 4/50] [Batch 648/938] [D loss: 0.4110] [G loss: 2.7737]\n",
            "[Epoch 5/50] [Batch 110/938] [D loss: 0.1541] [G loss: 2.2486]\n",
            "[Epoch 5/50] [Batch 510/938] [D loss: 0.3268] [G loss: 1.3346]\n",
            "[Epoch 5/50] [Batch 910/938] [D loss: 0.2659] [G loss: 1.3640]\n",
            "[Epoch 6/50] [Batch 372/938] [D loss: 0.3486] [G loss: 1.0904]\n",
            "[Epoch 6/50] [Batch 772/938] [D loss: 0.2197] [G loss: 1.5525]\n",
            "[Epoch 7/50] [Batch 234/938] [D loss: 0.2523] [G loss: 2.8156]\n",
            "[Epoch 7/50] [Batch 634/938] [D loss: 0.1690] [G loss: 3.0441]\n",
            "[Epoch 8/50] [Batch 96/938] [D loss: 0.0740] [G loss: 2.6436]\n",
            "[Epoch 8/50] [Batch 496/938] [D loss: 0.1532] [G loss: 3.2875]\n",
            "[Epoch 8/50] [Batch 896/938] [D loss: 0.1673] [G loss: 3.0035]\n",
            "[Epoch 9/50] [Batch 358/938] [D loss: 0.1119] [G loss: 2.8525]\n",
            "[Epoch 9/50] [Batch 758/938] [D loss: 0.2481] [G loss: 1.0608]\n",
            "[Epoch 10/50] [Batch 220/938] [D loss: 0.1674] [G loss: 2.2899]\n",
            "[Epoch 10/50] [Batch 620/938] [D loss: 0.2769] [G loss: 1.3177]\n",
            "[Epoch 11/50] [Batch 82/938] [D loss: 0.1604] [G loss: 2.5594]\n",
            "[Epoch 11/50] [Batch 482/938] [D loss: 0.1568] [G loss: 2.9157]\n",
            "[Epoch 11/50] [Batch 882/938] [D loss: 0.0799] [G loss: 3.6036]\n",
            "[Epoch 12/50] [Batch 344/938] [D loss: 0.6581] [G loss: 5.8744]\n",
            "[Epoch 12/50] [Batch 744/938] [D loss: 0.2398] [G loss: 2.4520]\n",
            "[Epoch 13/50] [Batch 206/938] [D loss: 0.1674] [G loss: 2.4358]\n",
            "[Epoch 13/50] [Batch 606/938] [D loss: 0.2256] [G loss: 1.3306]\n",
            "[Epoch 14/50] [Batch 68/938] [D loss: 0.5109] [G loss: 5.4857]\n",
            "[Epoch 14/50] [Batch 468/938] [D loss: 0.4849] [G loss: 5.4518]\n",
            "[Epoch 14/50] [Batch 868/938] [D loss: 0.1443] [G loss: 3.2536]\n",
            "[Epoch 15/50] [Batch 330/938] [D loss: 0.1274] [G loss: 2.3598]\n",
            "[Epoch 15/50] [Batch 730/938] [D loss: 0.1030] [G loss: 3.3460]\n",
            "[Epoch 16/50] [Batch 192/938] [D loss: 0.2576] [G loss: 7.2286]\n",
            "[Epoch 16/50] [Batch 592/938] [D loss: 0.1299] [G loss: 3.3529]\n",
            "[Epoch 17/50] [Batch 54/938] [D loss: 0.3276] [G loss: 8.0803]\n",
            "[Epoch 17/50] [Batch 454/938] [D loss: 0.1905] [G loss: 2.0014]\n",
            "[Epoch 17/50] [Batch 854/938] [D loss: 0.1316] [G loss: 2.3510]\n",
            "[Epoch 18/50] [Batch 316/938] [D loss: 0.2149] [G loss: 5.7420]\n",
            "[Epoch 18/50] [Batch 716/938] [D loss: 0.5333] [G loss: 1.1213]\n",
            "[Epoch 19/50] [Batch 178/938] [D loss: 0.4524] [G loss: 0.6551]\n",
            "[Epoch 19/50] [Batch 578/938] [D loss: 0.4466] [G loss: 0.6197]\n",
            "[Epoch 20/50] [Batch 40/938] [D loss: 0.1078] [G loss: 2.8605]\n",
            "[Epoch 20/50] [Batch 440/938] [D loss: 0.2075] [G loss: 5.5103]\n",
            "[Epoch 20/50] [Batch 840/938] [D loss: 0.1359] [G loss: 2.6591]\n",
            "[Epoch 21/50] [Batch 302/938] [D loss: 0.3100] [G loss: 1.1939]\n",
            "[Epoch 21/50] [Batch 702/938] [D loss: 0.2702] [G loss: 4.2157]\n",
            "[Epoch 22/50] [Batch 164/938] [D loss: 0.5316] [G loss: 8.4112]\n",
            "[Epoch 22/50] [Batch 564/938] [D loss: 0.1097] [G loss: 3.2512]\n",
            "[Epoch 23/50] [Batch 26/938] [D loss: 0.2212] [G loss: 2.0748]\n",
            "[Epoch 23/50] [Batch 426/938] [D loss: 0.1828] [G loss: 3.2999]\n",
            "[Epoch 23/50] [Batch 826/938] [D loss: 0.1304] [G loss: 2.3123]\n",
            "[Epoch 24/50] [Batch 288/938] [D loss: 0.1428] [G loss: 2.0259]\n",
            "[Epoch 24/50] [Batch 688/938] [D loss: 0.1485] [G loss: 2.4439]\n",
            "[Epoch 25/50] [Batch 150/938] [D loss: 0.1333] [G loss: 2.0527]\n",
            "[Epoch 25/50] [Batch 550/938] [D loss: 0.1938] [G loss: 3.7111]\n",
            "[Epoch 26/50] [Batch 12/938] [D loss: 0.2049] [G loss: 3.2456]\n",
            "[Epoch 26/50] [Batch 412/938] [D loss: 0.2358] [G loss: 5.0062]\n",
            "[Epoch 26/50] [Batch 812/938] [D loss: 0.1727] [G loss: 4.9810]\n",
            "[Epoch 27/50] [Batch 274/938] [D loss: 0.1013] [G loss: 2.4364]\n",
            "[Epoch 27/50] [Batch 674/938] [D loss: 0.0992] [G loss: 2.7726]\n",
            "[Epoch 28/50] [Batch 136/938] [D loss: 0.2544] [G loss: 2.6009]\n",
            "[Epoch 28/50] [Batch 536/938] [D loss: 0.2815] [G loss: 5.5949]\n",
            "[Epoch 28/50] [Batch 936/938] [D loss: 0.1311] [G loss: 7.2114]\n",
            "[Epoch 29/50] [Batch 398/938] [D loss: 0.1128] [G loss: 2.6590]\n",
            "[Epoch 29/50] [Batch 798/938] [D loss: 0.1231] [G loss: 3.0181]\n",
            "[Epoch 30/50] [Batch 260/938] [D loss: 0.1379] [G loss: 3.9138]\n",
            "[Epoch 30/50] [Batch 660/938] [D loss: 0.1542] [G loss: 2.6792]\n",
            "[Epoch 31/50] [Batch 122/938] [D loss: 0.4042] [G loss: 4.1319]\n",
            "[Epoch 31/50] [Batch 522/938] [D loss: 0.1369] [G loss: 2.6949]\n",
            "[Epoch 31/50] [Batch 922/938] [D loss: 0.1570] [G loss: 1.8562]\n",
            "[Epoch 32/50] [Batch 384/938] [D loss: 0.0574] [G loss: 2.9228]\n",
            "[Epoch 32/50] [Batch 784/938] [D loss: 0.1983] [G loss: 2.8736]\n",
            "[Epoch 33/50] [Batch 246/938] [D loss: 0.8377] [G loss: 8.7336]\n",
            "[Epoch 33/50] [Batch 646/938] [D loss: 0.1881] [G loss: 3.3256]\n",
            "[Epoch 34/50] [Batch 108/938] [D loss: 0.3126] [G loss: 4.8035]\n",
            "[Epoch 34/50] [Batch 508/938] [D loss: 0.1974] [G loss: 1.4818]\n",
            "[Epoch 34/50] [Batch 908/938] [D loss: 0.0737] [G loss: 3.0419]\n",
            "[Epoch 35/50] [Batch 370/938] [D loss: 0.1257] [G loss: 2.4695]\n",
            "[Epoch 35/50] [Batch 770/938] [D loss: 0.0828] [G loss: 3.0045]\n",
            "[Epoch 36/50] [Batch 232/938] [D loss: 0.2226] [G loss: 1.3853]\n",
            "[Epoch 36/50] [Batch 632/938] [D loss: 0.0887] [G loss: 2.3921]\n",
            "[Epoch 37/50] [Batch 94/938] [D loss: 0.0985] [G loss: 4.1814]\n",
            "[Epoch 37/50] [Batch 494/938] [D loss: 0.2473] [G loss: 3.4426]\n",
            "[Epoch 37/50] [Batch 894/938] [D loss: 0.1240] [G loss: 4.2523]\n",
            "[Epoch 38/50] [Batch 356/938] [D loss: 0.1301] [G loss: 2.8975]\n",
            "[Epoch 38/50] [Batch 756/938] [D loss: 0.1540] [G loss: 4.2661]\n",
            "[Epoch 39/50] [Batch 218/938] [D loss: 0.2404] [G loss: 1.3375]\n",
            "[Epoch 39/50] [Batch 618/938] [D loss: 0.2050] [G loss: 1.9956]\n",
            "[Epoch 40/50] [Batch 80/938] [D loss: 0.1285] [G loss: 3.3537]\n",
            "[Epoch 40/50] [Batch 480/938] [D loss: 0.2590] [G loss: 2.9071]\n",
            "[Epoch 40/50] [Batch 880/938] [D loss: 0.0975] [G loss: 2.8045]\n",
            "[Epoch 41/50] [Batch 342/938] [D loss: 0.1703] [G loss: 2.2378]\n",
            "[Epoch 41/50] [Batch 742/938] [D loss: 0.0868] [G loss: 2.6503]\n",
            "[Epoch 42/50] [Batch 204/938] [D loss: 0.0951] [G loss: 3.3317]\n",
            "[Epoch 42/50] [Batch 604/938] [D loss: 0.0888] [G loss: 5.2154]\n",
            "[Epoch 43/50] [Batch 66/938] [D loss: 0.0609] [G loss: 7.0531]\n",
            "[Epoch 43/50] [Batch 466/938] [D loss: 0.1135] [G loss: 2.4497]\n",
            "[Epoch 43/50] [Batch 866/938] [D loss: 0.2588] [G loss: 2.8940]\n",
            "[Epoch 44/50] [Batch 328/938] [D loss: 0.0915] [G loss: 3.9325]\n",
            "[Epoch 44/50] [Batch 728/938] [D loss: 0.1729] [G loss: 5.0486]\n",
            "[Epoch 45/50] [Batch 190/938] [D loss: 0.1214] [G loss: 2.3522]\n",
            "[Epoch 45/50] [Batch 590/938] [D loss: 0.1337] [G loss: 3.2093]\n",
            "[Epoch 46/50] [Batch 52/938] [D loss: 0.3248] [G loss: 5.2648]\n",
            "[Epoch 46/50] [Batch 452/938] [D loss: 0.0849] [G loss: 4.2783]\n",
            "[Epoch 46/50] [Batch 852/938] [D loss: 0.3789] [G loss: 7.5038]\n",
            "[Epoch 47/50] [Batch 314/938] [D loss: 0.1054] [G loss: 2.6327]\n",
            "[Epoch 47/50] [Batch 714/938] [D loss: 0.1151] [G loss: 2.5686]\n",
            "[Epoch 48/50] [Batch 176/938] [D loss: 0.1965] [G loss: 1.7184]\n",
            "[Epoch 48/50] [Batch 576/938] [D loss: 0.1515] [G loss: 2.2397]\n",
            "[Epoch 49/50] [Batch 38/938] [D loss: 0.1810] [G loss: 2.0505]\n",
            "[Epoch 49/50] [Batch 438/938] [D loss: 0.1236] [G loss: 3.5810]\n",
            "[Epoch 49/50] [Batch 838/938] [D loss: 0.0313] [G loss: 7.4723]\n",
            "¡Entrenamiento completado! Modelo guardado como 'generador_mnist.pth'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# BLOQUE 6\n",
        "\n",
        "!pip install gradio\n",
        "\n",
        "import gradio as gr\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn.functional as F  # Importamos funciones para redimensionar\n",
        "\n",
        "# 1. Cargar el modelo (Igual que antes)\n",
        "def cargar_modelo():\n",
        "    modelo = Generator()\n",
        "    # Asegúrate de que el archivo .pth exista en la carpeta\n",
        "    if torch.cuda.is_available():\n",
        "        modelo.load_state_dict(torch.load('generador_mnist.pth'))\n",
        "        modelo.cuda()\n",
        "    else:\n",
        "        modelo.load_state_dict(torch.load('generador_mnist.pth', map_location=torch.device('cpu')))\n",
        "    modelo.eval()\n",
        "    return modelo\n",
        "\n",
        "generador_entrenado = cargar_modelo()\n",
        "\n",
        "# 2. Función de Generación con \"LUPA\" (Zoom)\n",
        "def generar_digito(semilla):\n",
        "    # Convertir semilla\n",
        "    seed = int(semilla)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    # Generar ruido y procesar en el dispositivo correcto (CPU o GPU)\n",
        "    device = next(generador_entrenado.parameters()).device\n",
        "    z = torch.randn(1, 100).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Generamos la imagen original (Tamaño 28x28)\n",
        "        img_tensor = generador_entrenado(z)\n",
        "\n",
        "        # --- AQUÍ ESTÁ EL TRUCO DEL ZOOM ---\n",
        "        # Usamos interpolate para agrandar la imagen 10 veces (de 28 a 280 px)\n",
        "        # mode='nearest' mantiene los bordes definidos (estilo pixel art) para que no se vea borroso.\n",
        "        img_tensor = F.interpolate(img_tensor, scale_factor=10, mode='nearest')\n",
        "\n",
        "    # Procesar para mostrar\n",
        "    img_array = img_tensor.cpu().view(280, 280).numpy() # Ahora es 280x280\n",
        "    img_array = (img_array + 1) / 2\n",
        "\n",
        "    return img_array\n",
        "\n",
        "# 3. Interfaz\n",
        "interfaz = gr.Interface(\n",
        "    fn=generar_digito,\n",
        "    inputs=[\n",
        "        gr.Slider(minimum=0, maximum=1000, value=42, step=1, label=\"Código de Variación (Semilla)\")\n",
        "    ],\n",
        "    outputs=gr.Image(label=\"Dígito Generado (Zoom 10x)\", type=\"numpy\"), # Especificamos salida de imagen\n",
        "    title=\"Generador de Datos Sintéticos MNIST (Alta Visibilidad)\",\n",
        "    description=\"Herramienta de IA Generativa. Mueve el slider para generar dígitos manuscritos sintéticos. Se ha aplicado un escalado 10x para facilitar la visualización.\",\n",
        "    theme=\"default\"\n",
        ")\n",
        "\n",
        "interfaz.launch(share=True, debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "O33hcVRqb-jV",
        "outputId": "dbc2c7ae-a220-469e-fbc0-7b9bc1ec62b4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.12/dist-packages (5.50.0)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.11.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.2.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.118.3)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio) (1.0.0)\n",
            "Requirement already satisfied: gradio-client==1.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.14.0)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.33.5 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.36.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.0.3)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.11.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from gradio) (25.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (11.3.0)\n",
            "Requirement already satisfied: pydantic<=2.12.3,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.12.3)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (6.0.3)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.14.6)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.7)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.48.0)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.20.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.15.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.38.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.14.0->gradio) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=13.0 in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.14.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (3.11)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (3.20.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (1.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<=2.12.3,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<=2.12.3,>=2.0->gradio) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<=2.12.3,>=2.0->gradio) (0.4.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (8.3.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<2.0,>=0.33.5->gradio) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<2.0,>=0.33.5->gradio) (2.5.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://41ee4ca2c0e41a1677.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://41ee4ca2c0e41a1677.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created dataset file at: .gradio/flagged/dataset2.csv\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://41ee4ca2c0e41a1677.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    }
  ]
}